# belief-agent
An explicit uncertainty-aware cognitive agent that models latent belief states and performs inference given incomplete information. 

When AI components communicate under uncertainty, naive information sharing can make systems worse. 
This repository explores and studies when communication helps—and when it causes cascading failure.

## Project Structure

This project is organized into a modular system:

- **Branch A — Belief Inference Under Uncertainty**  
  Explicit belief representations, entropy-based uncertainty, and communication between agents.

- **Branch B — Associative Memory**  
  Memory systems that store and retrieve belief–outcome associations with attractor-like dynamics.

- **Branch C — Information Integration Across Time**  
  Intend to explore temporal stability, recovery from perturbation, and integration vs fragmentation.

- **Branch D — Uncertainty-Weighted Learning**  
  Intend to explore confidence-modulated updates and comparisons to standard error-driven learning.

- **Branch E — Self-Modeling Agents**  
  Intend to produce agents that represent uncertainty about their own reliability and adjust behavior accordingly.


Current status:
- Branch A: Belief inference under uncertainty (complete)
- Branch B: Associative memory (in progress)

Details, results, and analysis are documented inline in code and experiment logs.
A full README will be written once all branches are complete.
